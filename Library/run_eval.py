from . import hardware_usage, format_input, llamacpp as llamalib
from llama_cpp import Llama
from datasets import Dataset, load_dataset, concatenate_datasets
from tqdm import tqdm
import json
import threading
from datetime import datetime
import time
import os
import re

SYSTEM = """### IDENTITY and PURPOSE

- **Your Task:** Derive Your Final Answer by Choosing the Correct Option for a Multi-Choice Question.

### INSTRUCTION STEPS:

* Carefully read the question and its options.
* Identify key phrases or words that can guide your decision.
* Eliminate any options that are clearly incorrect based on your knowledge.
* Compare the remaining options to determine which one best answers the question.

### OUTPUT INSTRUCTIONS

- Select only one option as your final answer.
- Only output the letter of the correct option.
- Keep your answer under 2048 tokens.
- Please output the answer in the format of \"The answer is (X)\" at the end.

Ensure you select only one correct answer based on your knowledge and expertise."""

SYSTEM = """### IDENTITY and PURPOSE

- **Your Task:** Derive Your Final Answer {}.

### INSTRUCTION STEPS:

* {}.
* Identify key phrases or words that can guide your decision.
* Eliminate any options that are clearly incorrect based on your knowledge.
* Compare the remaining options to determine which one best answers the question.

### OUTPUT INSTRUCTIONS
{}{}
- **Keep your answer under 2048 tokens.**
- Avoid asking for clarification or ask questions.
- Avoid any follow-up questions or offers for further assistance.
- Always solve the problem and calculations, even if you are unsure or lack information.
- Please output the answer in the format of \"The {} is (X).\" at the end.
{}
Ensure you select only one correct answer based on your knowledge and expertise."""

hardware_logging = True

OUTPUT_SPACE = list('ABCDEFGHIJKLMNOP')

def extract_leftover(text: str) -> str:
    """
    This function extracts potential leftover answer characters from a text response. 

    Args:
        text (str): The text response generated by the model.

    Returns:
        str: A string containing any leftover answer characters found in the response.
    """
    left_over = ""
    text_pre = text
    replace_chars = ['.', '(', ')', '**', '*', '\n', ' ', '$']
    for c in replace_chars:
        text = text.replace(c, "|")
    response_words = text
    for word in response_words.split('|'):
        if len(word) < 2 and word in OUTPUT_SPACE:
            left_over += word + " "
    resp = left_over.strip()
    #print(f'Response:\n{text_pre}\nExtracted answer:\n{resp}')
    return resp

def extract_answer(text: str) -> str:
    """
    This function extracts the chosen answer from a given text string.

    It first attempts to find patterns indicating the answer format, such as "Answer is (A)" or "The option is B". 
    If no clear pattern is found, it resorts to extracting individual characters and attempting to identify a single letter within the remaining text.

    Args:
        text (str): The input text string containing the answer.

    Returns:
        str: The extracted answer letter as a single character string.
    """
    text = text.replace('\n\n', ' ').replace('\n', ' ').replace(':', '').replace('*', '').replace('**', '')
    text = ' '.join(text.split())
        
    # pattern = r'(?:nswer|ption|orrect|olution|olution\s+is|hoose|onclusion)(?:\s+(?:choice|is))?[:.\s]*(?:\*{0,2}|\()?([a-zA-Z])(?:\){0,1}|\*{0,2})(?!\w)'
    # pattern = r'(?:answer|option|correct|solution|choose|conclusion)(?:\s+is)?.*?([a-zA-Z]).*?\)?'
    pattern = r'(?:answer|option|correct|solution|choose|conclusion)(?:\s+is)?.*?(\(?[a-zA-Z]\)?)'
    match = re.search(pattern, text, re.IGNORECASE)
    if match:
        extracted_letter = match.group(1).strip()
        return extract_leftover(extracted_letter)
    else:
        pattern = r'(?:.*?[(:]\s*)([A-G])\s*(?:\)|\.|$)'
        match = re.search(pattern, text)
        if match:
            extracted_letter = match.group(1).strip()
            return extract_leftover(extracted_letter)
    
    # fails to find the answer, tries to extract it anyway by removing text and characters
    return extract_leftover(text)

def basic_extract(text: str) -> str:
    patterns = [
        r'(?:answer|output|option|correct|solution|choose|conclusion)(?:\s+is)?\s*[\'(]([^)]*)[\'\)]',
        r'(?:answer|output|option|correct|solution|choose|conclusion)(?:\s+is)?\s*(\{[^}]+\})',
        r'(?:answer|output|option|correct|solution|choose|conclusion)(?:\s+is)?\s*(.*)'
    ] # lessens the constraints on what is extracted
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return '[ERROR]'

def load_eval_data(data_name: str):
    """
    Load the evaluation dataset from Hugging Face datasets using the provided dataset name.

    This function takes a string as input, which represents the name of a dataset available on the Hugging Face datasets library. It uses this name to load the dataset and returns the test split of that dataset.

    Parameters:
        data_name (str): The name of the dataset to be loaded from Hugging Face datasets.

    Returns:
        dataset (datasets.Dataset): The test split of the loaded dataset.
    """
    dataset = load_dataset(data_name)

    # If the dataset is MMLU-Pro, we pick out 100 examples from each category
    if data_name == 'TIGER-Lab/MMLU-Pro':
        # Initialize an empty dictionary to store filtered data
        filtered = {}
        # Iterate over each example in the test dataset
        ds1 = dataset['test']
        for d in ds1:
            # If this category is not already in our filtered data, add it
            if d['category'] not in filtered:
                filtered[d['category']] = []
            # If we have less than 100 examples for this category, add this one to the list
            if len(filtered[d['category']]) < 100:
                filtered[d['category']].append(d)
        # Concatenate the datasets from each category and return it along with the validation dataset
        conc = concatenate_datasets([ Dataset.from_list(filtered[x]) for x in filtered])
        return conc, dataset['validation']
    # If the dataset is cruxeval, we simply return the test dataset
    elif data_name == 'cruxeval-org/cruxeval':
        return dataset['test']
    # If the dataset is MuSR, we add a 'type' column to each split and concatenate them
    elif data_name == 'TAUR-Lab/MuSR':
        # Add 'type' column to each split of the dataset
        d1 = dataset['murder_mysteries']
        d1 = d1.add_column('type', [ 'murder_mysteries' for i in range(len(d1))])

        d2 = dataset['object_placements']
        d2 = d2.add_column('type', [ 'object_placements' for i in range(len(d2))])

        d3 = dataset['team_allocation']
        d3 = d3.add_column('type', [ 'team_allocation' for i in range(len(d3))])

        # Concatenate the datasets and add an 'id' column to keep track of each example
        final_set = concatenate_datasets([d1, d2, d3])
        final_set = final_set.add_column('id', [ i for i in range(len(final_set))])

        # Return the final dataset
        return final_set
        
"""This module provides functions for evaluating data using a Language Learning Model (LLM).
It includes the function evaluate_data which calculates the number of correct and incorrect answers,
and stores them in dictionaries categorized by answer category. It also logs the expected and generated answer,
along with the time taken to generate each answer, the total tokens used, and the tokens per second."""
def evaluate_data(llm, dataset_name, eval_data, total_questions, n_shot, system, model_name, dataset_settings):
    """Evaluate a dataset using a Language Learning Model (LLM).

    Args:
        llm: The language learning model to use for evaluation.
        dataset_name: The name of the dataset being evaluated.
        eval_data: The data to evaluate.
        total_questions: The total number of questions in the dataset.
        n_shot: The number of shots to use for few-shot prompting.
        system: The system message to use for chat completion.
        model_name: The name of the LLM model being used.
        dataset_settings: Settings specific to the dataset being evaluated.

    Returns:
        A tuple containing the total number of correct answers, the total number of wrong answers,
        a dictionary mapping answer categories to the number of correct answers in that category,
        a dictionary mapping answer categories to the number of wrong answers in that category,
        and a dictionary mapping question IDs to information about each question (expected answer, generated answer, time taken, tokens used, tokens per second).
    """
    # Initialize variables for tracking correct and incorrect answers
    shot_data = []
    correct_answers = 0
    wrong_answers = 0
    correct_answers_dict = {}
    wrong_answers_dict = {}
    question_dict = {}

    # Gather data for few-shot prompting
    for i in range(n_shot+1):
        shot_data.append(eval_data[i])

    # Shuffle the evaluation data and iterate over it
    eval_data = eval_data.shuffle(seed=137)
    for item in tqdm(eval_data, desc="Evaluating", total=total_questions):
        try:
            # Extract expected answer and category from the current data item
            expected_answer = item['answer']
            cat = item['category']
            question_id = item['question_id']

            # Initialize dictionaries for tracking correct and incorrect answers by category
            if cat not in correct_answers_dict:
                correct_answers_dict[cat] = 0
            if cat not in wrong_answers_dict:
                wrong_answers_dict[cat] = 0

            # Generate few-shot prompt and get model response
            cot_prompt = format_input.generate_cot_prompt(item, dataset_name)
            few_shot_messages = format_input.generate_few_shot_prompt(shot_data, item, system=system, model_name=model_name, dataset_name=dataset_name)
            few_shot_messages.append({'role': 'user', 'content': cot_prompt})

            response, tok_per_ms, delta_time_llama, total_tokens = llamalib.llama_cpp_call_messages(llm, few_shot_messages, dataset_settings)
            
            # Extract generated answer from model response
            left_over = extract_answer(response)
            if len(left_over) > 0:
                generated_answer = left_over.strip()[-1]

            # Compare expected and generated answers, update counters accordingly
            if generated_answer == expected_answer:
                correct_answers += 1
                correct_answers_dict[cat] += 1
            else:
                wrong_answers_dict[cat] += 1

            # Store information about the current question in a dictionary
            question_dict[question_id] = {
                'expected': expected_answer,
                'answer': generated_answer,
                'time': delta_time_llama,
                'tokens': total_tokens,
                'toks/sec': tok_per_ms,
            }
        except Exception as e:
            print(f"Error processing question {item['question']}: {e}")

    # Return evaluation results
    return correct_answers, wrong_answers, correct_answers_dict, wrong_answers_dict, question_dict

def process_mmlu_pro(item, model_name, dataset_name, llm, dataset_settings, shot_data):
    """
    Processes a single item from the MMLU-Pro dataset.

    Parameters:
        item (dict): A dictionary containing information about a single data point in the dataset.
                     It is expected to have an 'answer' key which contains the correct answer for the data point.
        model_name (str): The name of the language model being used.
        dataset_name (str): The name of the current dataset.
        llm: The language model object that is used for generating responses.
        dataset_settings (dict): A dictionary containing settings specific to the current dataset.
        shot_data (list): A list of previous data points in the dataset that will be used as few-shot examples.

    Returns:
        tuple: A tuple containing a dictionary with information about the results and a string with the generated answer.
               The dictionary contains the expected answer, the generated answer, execution time, number of tokens used,
               and tokens processed per second.
    """
    # Extract the correct answer from the current data point
    expected_answer = item['answer']

    # Generate a prompt for the language model that includes the current data point's question and choices
    cot_prompt = format_input.generate_cot_prompt(item, dataset_name)

    # Generate few-shot prompts from previous data points
    few_shot_messages = format_input.generate_few_shot_prompt(shot_data, item, system=SYSTEM, model_name=model_name, dataset_name=dataset_name)

    # Append the current data point's prompt to the few-shot prompts
    few_shot_messages.append({'role': 'user', 'content': cot_prompt})

    # Call the language model with the generated messages and get the response, tokens per second, time taken, and total tokens used
    response, tok_per_ms, delta_time_llama, total_tokens = llamalib.llama_cpp_call_messages(llm, few_shot_messages, dataset_settings)

    # Extract the answer from the language model's response
    left_over = extract_answer(response)

    # If no answer was extracted, set the generated answer to an empty string
    if left_over == '':
        generated_answer = ''
    # If an answer was extracted, strip any whitespace and get the last character of the answer (assumed to be a single letter)
    elif len(left_over) > 0:
        generated_answer = left_over.strip()
        if len(generated_answer) > 0:
            generated_answer = generated_answer[-1]

    # Create a dictionary with the expected answer, generated answer, time taken, tokens used, and tokens processed per second
    answer_set = {
        'expected': expected_answer,
        'answer': generated_answer,
        'time': delta_time_llama,
        'tokens': total_tokens,
        'toks/sec': tok_per_ms,
    }

    # Return the dictionary and the generated answer
    return answer_set, generated_answer
    """try:
        
    
    except Exception as e:
        print(f"Error processing question {item['question']}: {e}")
    return None"""

def eval(eval_data: Dataset, validation_data, model_name: str, model_path: str, dataset_name: str, dataset_settings: dict):
    # If the evaluation data is not a dictionary, it shuffles it with a fixed seed for reproducibility.
    if type(eval_data) is not dict:
        eval_data = eval_data.shuffle(seed=137)

    # Limit due to resource limitations
    # if len(eval_data) > 1000:
    #     eval_data = eval_data.take(1000)

    # Initializing variables to keep track of the total questions and correct answers.
    total_questions = len(eval_data)
    correct_answers = 0
    correct_answers_dict = dict()
    wrong_answers_dict = dict()
    question_dict = dict()
    
    # Setting the input format based on the model name.
    # https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_chat_format.py
    if 'gemma' in model_name:
        inp_format = 'gemma'
    if 'llama3' in model_name:
        inp_format = 'llama-3'
    
    # Setting the number of GPU layers based on the model path.
    ngl = -1
    if '70b' in model_path:
        if 'Q2' in model_path:
            ngl = 80
        elif 'Q4' in model_path:
            ngl = 42
        elif 'Q6' in model_path:
            ngl = 30
        elif 'Q8' in model_path:
            ngl = 24
    if '27b' in model_path:
        if 'Q8' in model_path:
            ngl = 35
    
    # Initializing the LLM with the specified parameters.
    llm = Llama(model_path=model_path, 
            n_ctx=4096,
            n_batch=512,
            n_gpu_layers=ngl,
            #n_gpu_layers=31, # llama 3 70b Q2: 67, llama 3 70b Q4: 42, llama 3 70b Q6: 30
            verbose=True,
            flash_attn=True,
            n_threads_batch=16,
            n_threads=16,
            offload_kqv=True
        )
    """if 'gemma' in model_name:
        llm = Llama(model_path=model_path, 
            n_ctx=4096,
            n_batch=512,
            n_gpu_layers=-1,
            verbose=True,
            format='gemma',
            flash_attn=True,
            n_threads_batch=16,
            n_threads=16,
            offload_kqv=True
            )
    else:
        llm = Llama(model_path=model_path, 
            n_ctx=4096,
            n_batch=512,
            n_gpu_layers=-1,
            verbose=True,
            flash_attn=True,
            n_threads_batch=16,
            n_threads=16,
            offload_kqv=True
        )"""
    # Getting the number of shots from the dataset settings.
    n_shot = dataset_settings['n_shot']

    shot_data = dict()
    if validation_data is not None:
        for item in validation_data:
            if item["category"] not in shot_data:
                shot_data[item["category"]] = []
            if len(shot_data[item["category"]]) < n_shot: #and 'wikipedia' not in item['cot_content'].lower():
                shot_data[item["category"]].append(item)

    global SYSTEM
    # Setting the system prompt based on the dataset name.
    if dataset_name == 'TIGER-Lab/MMLU-Pro':
        SYSTEM = SYSTEM.format('by Choosing the Correct Option for a Multi-Choice Question', 'Carefully read the question and its options', '\n- Select only one option as your final answer.', '\n- Only output the letter of the correct option.','answer', '**Replace X with your answer**.')
    elif dataset_name == 'cruxeval-org/cruxeval':
        SYSTEM = SYSTEM.format('based on the provided code and input', 'Carefully read the code and study how it works', '', '', 'output','**Replace X with your answer**.')
    elif dataset_name == 'TAUR-Lab/MuSR':
        SYSTEM = SYSTEM.format('by Choosing the Correct Option for a Multi-Choice Question after finishing reading the narrative', 'Carefully read the narrative, the question and its options', '\n- Select only one option as your final answer.', 'answer', 'output','**Replace X with your answer**.')

    # Adjusting the shot data based on the dataset name.
    if dataset_name == 'cruxeval-org/cruxeval':
        shot_data = []
        for i in range(n_shot+1):
            shot_data.append(eval_data[i])
    # If the dataset name is MuSR, it creates a dictionary with question types as keys and lists of questions as values.
    elif dataset_name == 'TAUR-Lab/MuSR':
        shot_data = {}
        for item in eval_data:
            if item['type'] not in shot_data:
                shot_data[item['type']] = []
            if len(shot_data[item['type']]) < n_shot+1:
                shot_data[item['type']].append(item)

    i = 1
    # It then iterates over each question in the evaluation data.
    for item in tqdm(eval_data, desc="Evaluating", total=total_questions):
        start_time = time.time()
        #if i > 2:
        #    break
        
        if dataset_name == 'TIGER-Lab/MMLU-Pro':
            # For each question, it processes the MMLU-Pro data and evaluates the model's answer against the expected answer.
            question_id = item['question_id']
            expected_answer = item['answer']
            cat = item['category']

            if cat not in correct_answers_dict:
                correct_answers_dict[cat] = 0
            if cat not in wrong_answers_dict:
                wrong_answers_dict[cat] = 0
            
            answer_set, generated_answer = process_mmlu_pro(item, model_name, dataset_name, llm, dataset_settings, shot_data)
            question_dict[question_id] = answer_set

            if generated_answer == expected_answer:
                correct_answers += 1
                correct_answers_dict[cat] += 1
            else:
                wrong_answers_dict[cat] += 1

            if i % 10 == 0 and i != 0:
                print(f"\nCorrect: {correct_answers} / {i} questions processed. ({round(correct_answers/i * 100)} %)")
                delta_time = round((time.time() - start_time)*1000)
                total_tokens = answer_set['tokens']
                tok_per_ms = answer_set['toks/sec']
                print(f"\nWhole iteration time: {delta_time} ms. Tokens: {total_tokens}. Tok/s: {round(tok_per_ms)}")
        
        elif dataset_name == 'cruxeval-org/cruxeval':
            # For each question, it processes the CruxEval data and evaluates the model's answer against the expected output.
            tmp_shot = shot_data.copy()
            for j, it in enumerate(shot_data):
                if it['id'] == item['id']:
                    #del it
                    del tmp_shot[j]
            if len(tmp_shot) == 6:
                tmp_shot = tmp_shot[:n_shot]
            code = item['code']
            inp = item['input']
            out = item['output']
            item_id = item['id']

            cot_prompt = format_input.generate_cot_prompt(item, dataset_name)
                
            few_shot_messages = format_input.generate_few_shot_prompt(tmp_shot, item, system=SYSTEM, model_name=model_name, dataset_name=dataset_name)
            few_shot_messages.append({'role': 'user', 'content': cot_prompt})


            response, tok_per_ms, delta_time_llama, total_tokens = llamalib.llama_cpp_call_messages(llm, few_shot_messages, dataset_settings)

            generated_answer = basic_extract(response)

            #if generated_answer == '[ERROR]':
            #    print(f"Error processing question:\n{item['question']}\n\nResponse: \n{response}")
            
            if 'O' not in correct_answers_dict:
                correct_answers_dict['O'] = 0
            if 'O' not in wrong_answers_dict:
                wrong_answers_dict['O'] = 0

            if generated_answer != '[ERROR]' and (generated_answer == out or generated_answer.strip() == out or generated_answer == out[1:-1] or generated_answer[1:-1] == out[1:-1] or generated_answer[1:-1] == out):
                correct_answers += 1
                correct_answers_dict['O'] += 1
            else:
                wrong_answers_dict['O'] += 1 # I later?

            if i % 10 == 0 and i != 0:
                print(f"\nCorrect: {correct_answers} / {i} questions processed. ({round(correct_answers/i * 100)} %)")
                delta_time = round((time.time() - start_time)*1000)
                print(f"\nWhole iteration time: {delta_time} ms. Tokens: {total_tokens}. Tok/ms: {round(tok_per_ms)}")
            
            question_dict[item_id] = {
                    'expected': out, 
                    'answer': generated_answer,
                    'time': delta_time_llama,
                    'tokens': total_tokens,
                    'toks/sec': tok_per_ms,
                }

        elif dataset_name == 'TAUR-Lab/MuSR':
            # For each question, it processes the MuSR data and evaluates the model's answer against the expected answer index.
            item_id = item['id']
            cat = item['type']
            expected_answer = chr(65 + (item['answer_index'] % 26))
            
            tmp_shot = shot_data[cat].copy()
            for j, it in enumerate(shot_data[cat]):
                if it['id'] == item_id:
                    del tmp_shot[j]

            if len(tmp_shot) == 2:
                tmp_shot = tmp_shot[:n_shot]

            cot_prompt = format_input.generate_cot_prompt(item, dataset_name)

            few_shot_messages = format_input.generate_few_shot_prompt(tmp_shot, item, system=SYSTEM, model_name=model_name, dataset_name=dataset_name)
            few_shot_messages.append({'role': 'user', 'content': cot_prompt})

            response, tok_per_ms, delta_time_llama, total_tokens = llamalib.llama_cpp_call_messages(llm, few_shot_messages, dataset_settings)

            generated_answer = extract_answer(response)
            if generated_answer == '':
                generated_answer = ''
            elif len(generated_answer) > 0:
                generated_answer = generated_answer.strip()[-1]

            question_dict[item_id] = {
                'expected': expected_answer,
                'answer': generated_answer,
                'time': delta_time_llama,
                'tokens': total_tokens,
                'toks/sec': tok_per_ms,
            }
            if cat not in correct_answers_dict:
                correct_answers_dict[cat] = 0
            if cat not in wrong_answers_dict:
                wrong_answers_dict[cat] = 0

            if generated_answer == expected_answer:
                correct_answers += 1
                correct_answers_dict[cat] += 1
            else:
                wrong_answers_dict[cat] += 1

            if i % 10 == 0 and i != 0:
                print(f"\nCorrect: {correct_answers} / {i} questions processed. ({round(correct_answers/i * 100)} %)")
                delta_time = round((time.time() - start_time)*1000)
                total_tokens = question_dict[item_id]['tokens']
                tok_per_ms = question_dict[item_id]['toks/sec']
                print(f"\nWhole iteration time: {delta_time} ms. Tokens: {total_tokens}. Tok/s: {round(tok_per_ms)}")
        
        i += 1
    # After evaluating all questions, it deletes the LLM object and calculates the accuracy.
    del llm
    accuracy = correct_answers / i
    
    print(f"Evaluation complete. Accuracy: {accuracy:.2%}")
    return accuracy, correct_answers_dict, wrong_answers_dict, question_dict

def save_results(model_name, dataset_name, accuracy, correct_answers_dict, wrong_answers_dict, question_dict):
    """
    This function saves the results of a model evaluation to JSON files.

    Parameters:
        model_name (str): The name of the language model being evaluated.
        dataset_name (str): The name of the dataset used for evaluation.
        accuracy (float): The overall accuracy of the model on the dataset.
        correct_answers_dict (dict): A dictionary containing the number of correct answers by category.
        wrong_answers_dict (dict): A dictionary containing the number of incorrect answers by category.
        question_dict (dict): A dictionary containing the expected and generated answer for each question in the dataset.

    Returns:
        None
    """
    # The model's evaluation results for a specific dataset are being saved into JSON files.
    # First, we replace any forward slashes in the dataset name with hyphens for file-naming compatibility.
    dataset_name = dataset_name.replace('/', '-')
    # We then check if a 'results' directory exists; if not, we create one.
    if not os.path.exists('results'):
        os.mkdir('results')
    
    # Next, we check for the existence of a directory named after the model within the 'results' directory.
    # If it doesn't exist, we create this directory.
    if not os.path.exists(f'results/{model_name}'):
        os.mkdir(f'results/{model_name}')
    
    # Here, we open a file in write mode to save the model's evaluation results for the dataset.
    # The file is named using the model name and the dataset name, with '-results' appended at the end.
    # This file will contain the accuracy of the model on the dataset, as well as dictionaries containing correct and incorrect answers.
    with open(f'results/{model_name}/{model_name}-{dataset_name}-results.json', 'w+') as f:
        json.dump({'accuracy': accuracy, 'correct': correct_answers_dict, 'wrong': wrong_answers_dict}, f, indent=4, separators=(',', ': '))
    
    # Separately, we open another file in write mode to save the questions associated with the dataset.
    # The file is named using a similar convention as the results file, but '-questions' is appended at the end instead.
    # This file will contain all the questions used for evaluation.
    with open(f'results/{model_name}/{model_name}-{dataset_name}-questions.json', 'w+') as f:
        json.dump(question_dict, f, indent=4, separators=(',', ': '))


def log_hardware(logfiles: dict):
    """
    Logs hardware usage (power, RAM, and VRAM) to separate log files in a loop until 'hardware_logging' is False.

    Parameters:
        logfiles (dict): A dictionary containing the file paths for logging power, RAM, and VRAM usage.
                          The keys should be 'power', 'ram', and 'vram'.
    """
    # Check if power logging file exists in the dictionary of logfiles
    if 'power' in logfiles:
        # Open the power logging file in append mode and write an opening bracket for a list representation
        with open(logfiles['power'], 'a') as f:
            f.write('[')

    # Open the RAM logging file in append mode and write an opening bracket for a list representation
    with open(logfiles['ram'], 'a') as f:
        f.write('[')

    # Open the VRAM logging file in append mode and write an opening bracket for a list representation
    with open(logfiles['vram'], 'a') as f:
        f.write('[')

    # While hardware logging is enabled, perform the following actions
    while hardware_logging:
        # Get the current timestamp in a readable format
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # If power logging file exists, get the power usage and write it to the file along with the timestamp
        if 'power' in logfiles:
            power = {timestamp : hardware_usage.get_power()}
            with open(logfiles['power'], 'a') as f:
                f.write(f'{power},')

        # Get the RAM usage and write it to the file along with the timestamp
        ram = {timestamp : hardware_usage.get_ram_usage()}
        with open(logfiles['ram'], 'a') as f:
            f.write(f'{ram},')

        # Get the VRAM usage and write it to the file along with the timestamp
        vram = {timestamp : hardware_usage.get_vram_usage()}
        with open(logfiles['vram'], 'a') as f:
            f.write(f'{vram},')

        # Wait for 1 second before taking another set of readings
        time.sleep(1)

    # If power logging file exists, write a closing bracket to the file to signify the end of the list representation
    if 'power' in logfiles:
        with open(logfiles['power'], 'a') as f:
            f.write(']')

    # Write a closing bracket to the RAM and VRAM logging files to signify the end of their list representations
    with open(logfiles['ram'], 'a') as f:
        f.write(']')

    with open(logfiles['vram'], 'a') as f:
        f.write(']')

def start_logging_thread(model_name: str):
    """
    This function starts a separate thread to log hardware usage during the evaluation of a language model.

    Parameters:
        model_name (str): The name of the language model being evaluated. This is used to create unique log files for each model.

    Returns:
        None

    The function performs the following steps:
    1. Check if a 'results' directory exists in the current working directory. If it doesn't, create one.
    2. Within the 'results' directory, check if there is a subdirectory with the name of the language model being evaluated. If not, create one.
    3. Define the paths for the log files that will be created for RAM usage and VRAM usage. These file paths are relative to the model-specific subdirectory within the 'results' directory.
    4. Create a new thread that runs the `log_hardware` function. The `log_hardware` function is expected to take in a dictionary of log file paths as an argument. In this case, we pass in the dictionary containing the log files for RAM usage and VRAM usage.
    5. Start the thread. The `daemon=True` argument means that the thread will terminate when the main program exits.
    """
    # Check if a 'results' directory exists and create one if it doesn't
    if not os.path.exists(f'results'):
        os.mkdir(f'results')

    # Within the 'results' directory, check if there is a subdirectory with the name of the language model being evaluated
    # If not, create one
    if not os.path.exists(f'results/{model_name}'):
        os.mkdir(f'results/{model_name}')

    # Define the paths for the log files to be created for power usage, RAM usage, and VRAM usage
    logfiles = {
        'ram': f'./results/{model_name}/{model_name}_ram.json',
        'vram': f'./results/{model_name}/{model_name}_vram.json'
    }

    # Start a new thread that runs the `log_hardware` function, passing in the dictionary of log file paths as an argument
    # The thread is set to daemon mode so it will terminate when the main program exits
    thread = threading.Thread(target=log_hardware, args=(logfiles,), daemon=True)
    thread.start()

def evaluate(model_name: str, dataset_name: str, dataset_settings: dict, model_path: str = None):
    """
    Evaluates a given language model on a specified dataset and saves the results.

    Parameters:
        model_name (str): The name of the language model to be evaluated.
        dataset_name (str): The name of the dataset used for evaluation.
        model_path (str, optional): The path to the model's weights. If not provided, a default path is used.

    Returns:
        None
    """

    # Load the evaluation data for the specified dataset
    data = load_eval_data(dataset_name)

    # Start logging hardware usage in a separate thread (commented out as it's not implemented in this snippet)
    # start_logging_thread(model_name)

    # Separate evaluation and validation data if available
    eval_data, validation_data = data if len(data) == 2 else data[0], None

    # Evaluate the language model on the data and compute various metrics
    accuracy, correct_answers_dict, wrong_answers_dict, question_dict = eval(eval_data, validation_data, model_name, model_path, dataset_name, dataset_settings)

    # Set hardware logging flag to False (commented out as it's not used in this snippet)
    # hardware_logging = False

    # Save the results to a file in the 'results' directory
    save_results(model_name, dataset_name, accuracy, correct_answers_dict, wrong_answers_dict, question_dict)

"""
def main():

    MODELS = {
        'llama3': 'llama3:8b-instruct-q6_K', 
        'gemma2b': 'gemma:2b-instruct-v1.1-q8_0'
    }

    MODEL_PATH = {
        'gemma2b': './LLM/gemma-2b-it/gemma2b-it-Q4_K.gguf', 
    }

    MODEL = 'llama3:8b-instruct-q6_K'
    MODEL = 'gemma:2b-instruct-v1.1-q8_0'

    evaluate(MODEL, 'TIGER-Lab/MMLU-Pro', './LLM/gemma2b-v1.1-Q4_K_M.gguf')

    return

    logfile = f'{MODEL}_power_usage.log'
    thread = threading.Thread(target=log_power, args=(logfile,), daemon=True)
    thread.start()

    # Load the evaluation data
    eval_data, dataset_name = load_mmlu_pro()
    accuracy, correct_answers_dict, wrong_answers_dict, question_dict = eval(eval_data, MODEL)

    hardware_logging = False

    save_results(MODEL, dataset_name, accuracy, correct_answers_dict, wrong_answers_dict, question_dict)
    

if __name__ == '__main__':
  main()

"""
